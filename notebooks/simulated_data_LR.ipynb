{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import all of our files\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import fico\n",
    "import distribution_to_loans_outcomes as dlo\n",
    "\n",
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting parameters\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# this needs to be here so we can edit figures later\n",
    "plt.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cdfs, performance, totals = fico.get_FICO_data(data_dir=DATA_DIR);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdfs = all_cdfs[[\"White\",\"Black\"]]\n",
    "\n",
    "# B is White\n",
    "# A is Black\n",
    "\n",
    "cdf_B = cdfs['White'].values\n",
    "cdf_A = cdfs['Black'].values\n",
    "\n",
    "repay_B = performance['White']\n",
    "repay_A = performance['Black']\n",
    "\n",
    "scores = cdfs.index\n",
    "scores_list = scores.tolist()\n",
    "scores_repay = cdfs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to populate group distributions\n",
    "def get_pmf(cdf):\n",
    "    pis = np.zeros(cdf.size)\n",
    "    pis[0] = cdf[0]\n",
    "    for score in range(cdf.size-1):\n",
    "        pis[score+1] = cdf[score+1] - cdf[score]\n",
    "    return pis\n",
    "\n",
    "# to get loan repay probabilities for a given score\n",
    "loan_repaid_probs = [lambda i: repay_A[scores[scores.get_loc(i,method='nearest')]], \n",
    "                     lambda i: repay_B[scores[scores.get_loc(i,method='nearest')]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12066905 0.87933095]\n"
     ]
    }
   ],
   "source": [
    "# basic parameters\n",
    "N_scores = cdf_B.size\n",
    "N_groups = 2\n",
    "\n",
    "# get probability mass functions of each group\n",
    "pi_A = get_pmf(cdf_A)\n",
    "pi_B = get_pmf(cdf_B)\n",
    "pis = np.vstack([pi_A, pi_B])\n",
    "\n",
    "# demographic statistics \n",
    "group_ratio = np.array((totals[\"Black\"], totals[\"White\"]))\n",
    "group_size_ratio = group_ratio/group_ratio.sum()\n",
    "print(group_size_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get loan repay probabilities for a given score\n",
    "loan_repaid_probs = [lambda i: repay_A[scores[scores.get_loc(i,method='nearest')]], \n",
    "                     lambda i: repay_B[scores[scores.get_loc(i,method='nearest')]]]\n",
    "\n",
    "# unpacking repay probability as a function of score\n",
    "loan_repay_fns = [lambda x: loan_repaid_prob(x) for\n",
    "                      loan_repaid_prob in loan_repaid_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of the above is from Lydia's code in delayed-impact repos\n",
    "# all of the below is my code transforming the data and running linear regression on it!\n",
    "# NOTE: references for certain chunks will be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repay_probabilities(samples, repay_probs):\n",
    "    sample_probs = []\n",
    "    for index, score in enumerate(samples):\n",
    "        prob_index = np.where(scores_arr == score)\n",
    "        repay_prob = repay_probs[prob_index[0][0]]\n",
    "        sample_probs.insert(index, repay_prob)\n",
    "    return sample_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: much of the linear regression code is adapted from the below link\n",
    "#       https://realpython.com/linear-regression-in-python/\n",
    "# Problem with the below: https://datascience.stackexchange.com/questions/18258/how-to-force-weights-to-be-non-negative-in-linear-regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def run_linear_regression(x_arr, y_arr):\n",
    "    # data prep\n",
    "    x = x_arr.reshape(-1,1)\n",
    "    y = y_arr\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)\n",
    "    \n",
    "    # train model\n",
    "    model = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "    # analyze regression model on training set\n",
    "    # NOTE: the intercept, b0 value or coefficient, tells us what the regression predicts if the x is zero\n",
    "    #       the slope, b1 value, tells us how the predicted response rises/increases when x is increased by 1\n",
    "    # QUESTION: not sure if the input for score here should be our train and test x and y values? or our test values?\n",
    "    r_sq = model.score(x_train, y_train)\n",
    "    print('coefficient of determination:', r_sq)\n",
    "    print('intercept:', model.intercept_)\n",
    "    print('slope:', model.coef_)\n",
    "    \n",
    "    # get predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "    print('predicted response:', y_pred, sep='\\n')\n",
    "    \n",
    "    return model, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this function came from the Naomi Fridman's answer here \n",
    "#       https://stackoverflow.com/questions/26319259/how-to-get-a-regression-summary-in-python-scikit-like-r-does\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data in format needed\n",
    "# Make repay probabilities into percentages from decimals\n",
    "# NOTE: A is Black, B is White\n",
    "\n",
    "scores_arr = np.asarray(scores_list)\n",
    "repay_A_arr = pd.Series.to_numpy(repay_A)*100\n",
    "repay_B_arr = pd.Series.to_numpy(repay_B)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data according to the pmf\n",
    "# Reference: https://www.w3schools.com/python/ref_random_choices.asp\n",
    "\n",
    "from random import choices\n",
    "\n",
    "num_A_samples = 120\n",
    "num_B_samples = 880\n",
    "\n",
    "samples_A = np.asarray(sorted(choices(scores_arr, pi_A, k=num_A_samples)))\n",
    "samples_B = np.asarray(sorted(choices(scores_arr, pi_B, k=num_B_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata_A_dict = {'score': samples_A, 'repay_probability': samples_A_probs, 'race': samples_A_race}\\ndata_B_dict = {'score': samples_B, 'repay_probability': samples_B_probs, 'race': samples_B_race}\\n\\ndata_A_df = pd.DataFrame(data=data_A_dict, dtype=np.float64)\\ndata_B_df = pd.DataFrame(data=data_B_dict, dtype=np.float64)\\n\\ndata_all_df = pd.concat([data_A_df, data_B_df], ignore_index=True)\\nprint(data_all_df)\\ndata_all_df_shuffled = data_all_df.sample(frac=1).reset_index(drop=True)\\nprint(data_all_df_shuffled)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate samples groups' probabilities and make arrays for race\n",
    "# A == Black == 0 (later defined as 0.0 when converting to pandas df)\n",
    "# B == White == 1 (later defined as 1.0 when converting to pandas df)\n",
    "\n",
    "samples_A_probs = np.asarray(get_repay_probabilities(samples=samples_A, repay_probs=repay_A_arr))\n",
    "samples_A_race = np.asarray([0] * num_A_samples)\n",
    "\n",
    "samples_B_probs = np.asarray(get_repay_probabilities(samples=samples_B, repay_probs=repay_B_arr))\n",
    "samples_B_race = np.asarray([1] * num_B_samples)\n",
    "\n",
    "# If needed, I can use pandas df like below\n",
    "'''\n",
    "data_A_dict = {'score': samples_A, 'repay_probability': samples_A_probs, 'race': samples_A_race}\n",
    "data_B_dict = {'score': samples_B, 'repay_probability': samples_B_probs, 'race': samples_B_race}\n",
    "\n",
    "data_A_df = pd.DataFrame(data=data_A_dict, dtype=np.float64)\n",
    "data_B_df = pd.DataFrame(data=data_B_dict, dtype=np.float64)\n",
    "\n",
    "data_all_df = pd.concat([data_A_df, data_B_df], ignore_index=True)\n",
    "print(data_all_df)\n",
    "data_all_df_shuffled = data_all_df.sample(frac=1).reset_index(drop=True)\n",
    "print(data_all_df_shuffled)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the range of the Group A (Black) repay probabilities is:  94.86\n",
      "the min value is:  0.769999999999996\n",
      "the max value is:  95.63\n"
     ]
    }
   ],
   "source": [
    "# TROUBLESHOOTING: check range of repay probability values in array\n",
    "\n",
    "max_val = np.max(samples_A_probs)\n",
    "min_val = np.min(samples_A_probs)\n",
    "print('the range of the Group A (Black) repay probabilities is: ', max_val-min_val)\n",
    "print('the min value is: ', min_val)\n",
    "print('the max value is: ', max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.8981214374536469\n",
      "intercept: -96.74663213655134\n",
      "slope: [0.25379432]\n",
      "predicted response:\n",
      "[ 21.95102126  23.90328529 -14.56561271  33.04185796   3.86468944\n",
      "  35.45129775  41.07332393  -8.5228907   19.0226252   78.15076118\n",
      "  -2.17803258  33.84500456  -6.71007409  13.46590331  56.37594376\n",
      "  58.49089647  79.53007816 -17.58697372  55.26559359  72.63349325\n",
      "  35.45129775  22.92715328   6.41606096  39.46703074  19.99875722\n",
      "  -5.19939359  -3.68871308  90.6132958   45.35175278  63.24954005\n",
      "   3.86468944  46.0126755    2.35400893  61.66332552  33.04185796\n",
      "  80.44962282]\n"
     ]
    }
   ],
   "source": [
    "# Run linear regression on Group A -- Black\n",
    "\n",
    "model_A, y_test_A, y_pred_A = run_linear_regression(samples_A, samples_A_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mean Squared Logarithmic Error cannot be used when targets contain negative values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8dbd3f76c08c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# NOTE: can't get regression results because some predicted values are negative...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mregression_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-8fc61545707f>\u001b[0m in \u001b[0;36mregression_results\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmean_squared_log_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_log_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmedian_absolute_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mr2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_log_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         raise ValueError(\"Mean Squared Logarithmic Error cannot be used when \"\n\u001b[0m\u001b[1;32m    335\u001b[0m                          \"targets contain negative values.\")\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mean Squared Logarithmic Error cannot be used when targets contain negative values."
     ]
    }
   ],
   "source": [
    "# Analysis on Group A linear regression predictions\n",
    "# NOTE: can't get regression results because some predicted values are negative...\n",
    "\n",
    "regression_results(y_test_A, y_pred_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.9003970054749643\n",
      "intercept: -77.68698171233552\n",
      "slope: [0.23480279]\n",
      "predicted response:\n",
      "[ 70.82578172  59.8927769  108.44624108  68.37991934  55.00105215\n",
      " 101.0157731   94.96212713  84.54813306 109.93233467  12.60028083\n",
      "  81.57056147 101.7588199  118.80587774  70.82578172  96.68861822\n",
      "   7.00973825  88.74675921  52.55518977  33.93465137 103.98796029\n",
      " 100.6442497   81.1451941  108.44624108  23.19125314  15.39555211\n",
      " 101.3872965  109.56081127  26.45240298 101.0157731   23.19125314\n",
      "   9.80500954 109.18928787  29.41921313  81.57056147  97.37921466\n",
      "  64.46653954  66.42322944  24.27830309 109.56081127  90.4732503\n",
      "  90.81854852  37.54700195  67.89074687  47.59070836  40.90328718\n",
      "  40.90328718  26.45240298  93.92623248  66.42322944  89.78265387\n",
      "  90.81854852  89.09205743 110.56718343  96.68861822 117.77604095\n",
      "  88.401461    82.84666358  95.30742535  98.06981109  69.35826429\n",
      " 109.56081127  98.06981109  67.40157439 110.56718343  99.15815611\n",
      "  30.32230078  93.92623248  85.3988678  116.74620416 106.21710068\n",
      " 109.56081127  95.65272357  55.00105215  87.36556634  90.81854852\n",
      " 117.77604095  81.57056147  59.2813113  102.5018667  103.24491349\n",
      "  92.19974139  77.74225515  83.69739832  51.94372418  99.9012029\n",
      "  94.96212713  93.92623248  79.01835725 117.77604095  81.1451941\n",
      " 117.77604095  71.31495419 115.71636737  67.89074687  46.84766156\n",
      "  70.33660924 106.96014748  78.59298988  99.52967951  98.06981109\n",
      "  22.1042032   93.92623248  40.90328718  73.27164409  22.1042032\n",
      "  65.44488449  87.02026813 102.5018667   26.45240298 109.18928787\n",
      "  63.97736706  95.30742535  92.19974139  66.91240191  91.50914495\n",
      "  80.29445936  78.16762252  90.12795208 107.33167088  51.30594234\n",
      "  80.29445936  17.75600342  75.6154183  110.56718343  53.77812096\n",
      "  64.95571201  93.92623248 101.7588199   95.30742535 118.80587774\n",
      "  97.37921466  73.27164409   9.80500954   9.80500954 103.61643689\n",
      " 102.87339009  88.05616278  54.38958655  33.93465137  26.45240298\n",
      "  62.33863927  28.51612549  67.40157439  80.29445936  95.99802178\n",
      "  98.06981109  65.93405696  75.19005093  -1.65560273  17.75600342\n",
      " 108.07471768 100.2727263   84.97350043 106.58862408  31.22538843\n",
      " 100.2727263  113.65669379  80.71982673  88.05616278   8.4073739\n",
      "  28.51612549  -1.65560273  91.85444317 104.35948369  53.77812096\n",
      "  99.9012029   36.6439143   63.48819459  66.91240191   8.4073739\n",
      " 110.56718343  94.96212713  58.66984571  87.02026813  73.76081657\n",
      "  28.51612549  64.95571201 101.3872965   91.85444317 111.59702022\n",
      " 109.18928787 104.73100709 109.56081127  90.81854852 105.84557728\n",
      "  81.57056147 102.5018667  104.73100709  86.67496991  97.03391644\n",
      "  99.9012029   74.24998904 109.18928787 110.56718343  84.97350043\n",
      "  -1.65560273  53.77812096  43.87547437  77.74225515  32.12847607\n",
      " 111.59702022  81.57056147 103.61643689  75.19005093  66.42322944\n",
      "  55.61251774  96.68861822  -4.45087402 103.24491349  87.02026813\n",
      "  58.05838012  77.74225515  43.87547437  89.78265387 109.93233467\n",
      "  92.54503961  56.83544893  78.59298988  80.71982673  89.78265387\n",
      " 109.18928787  73.27164409 102.5018667   54.38958655  81.57056147\n",
      "  26.45240298  42.38938077  80.29445936  59.8927769   57.44691452\n",
      " 107.33167088  61.72717368  75.19005093  60.50424249  40.16024038\n",
      "  82.84666358  46.10461476  90.81854852  18.84305337 103.24491349\n",
      "  99.9012029   91.16384674  71.31495419  92.89033783  53.16665537\n",
      "  95.99802178  83.27203095  62.33863927  73.76081657  82.84666358\n",
      "  21.01715326  52.55518977 109.56081127 118.80587774  82.84666358\n",
      "  61.72717368  97.03391644  74.73916152  63.97736706  46.84766156\n",
      "  81.99592884  75.19005093  68.86909182 109.18928787]\n"
     ]
    }
   ],
   "source": [
    "# Run linear regression on Group B -- White\n",
    "\n",
    "model_B, y_test_B, y_pred_B = run_linear_regression(samples_B, samples_B_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mean Squared Logarithmic Error cannot be used when targets contain negative values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-38c5bb351e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# NOTE: can't get regression results because some predicted values are negative...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mregression_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-8fc61545707f>\u001b[0m in \u001b[0;36mregression_results\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmean_squared_log_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_log_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmedian_absolute_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mr2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_log_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         raise ValueError(\"Mean Squared Logarithmic Error cannot be used when \"\n\u001b[0m\u001b[1;32m    335\u001b[0m                          \"targets contain negative values.\")\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mean Squared Logarithmic Error cannot be used when targets contain negative values."
     ]
    }
   ],
   "source": [
    "# Analysis on Group B linear regression predictions\n",
    "# NOTE: can't get regression results because some predicted values are negative...\n",
    "\n",
    "regression_results(y_test_B, y_pred_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try the NNLS regression from scikit learn\n",
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_nnls.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try scipy non-negative least square regression\n",
    "# Reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html\n",
    "# TODO: look more into this if the above doesn't pan out...\n",
    "\n",
    "#from scipy.optimize import nnls\n",
    "\n",
    "#solution_vector, residual = nnls(x_train_B, x_test_B)\n",
    "#print(solution_vector)\n",
    "#print(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential TODO: could also try the below\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
