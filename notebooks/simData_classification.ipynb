{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. IMPORTANT--Specify classifier to be trained and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'svm_linear'  # options include: {'Decision Tree': 'dt', 'Gaussian Naive Bayes':'gnb',\\\n",
    "                           #                  'Logistic Regression': 'lgr', \\\n",
    "                           #                  'Support Vector Machine Linear': 'svm_linear', \\ \n",
    "                           #                  '??': 'auc', \\ \n",
    "                           #                  'Gradient_Boosted_Trees': 'gbt'} \n",
    "                           #                  gbt info: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "data_file = '/home/mackenzie/git_repositories/delayedimpact/data/simData_oom10.csv'  # ...oom10, ...oom50, ...oom100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from impt_functions import *\n",
    "import csv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, \\\n",
    "    balanced_accuracy_score, roc_auc_score\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from fairlearn.reductions import ExponentiatedGradient, GridSearch, DemographicParity, EqualizedOdds, \\\n",
    "    TruePositiveRateParity, FalsePositiveRateParity, ErrorRateParity, BoundedGroupLoss\n",
    "from fairlearn.metrics import *\n",
    "from raiwidgets import FairnessDashboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict used for saving overall results to a csv later\n",
    "# Example: overall = {'Unmitigated':[], 'EG DP': [], 'EG EO': [],\n",
    "#                   'EG EOO': [], 'EG FPRP': [], 'EG ERP': [],\n",
    "#                   'GS DP': [], 'GS EO': [],\n",
    "#                   'GS EOO': [], 'GS FPRP': [], 'GS ERP': []}\n",
    "# Example: black_results = {'Unmitigated Black': [], \n",
    "#                          'EG DP Black': [], ..., 'GS ERP Black': []}\n",
    "\n",
    "# Instantiate lists for holding results\n",
    "unmitigated, unmitigated_black, unmitigated_white = [], [], []\n",
    "\n",
    "eg_dp, eg_dp_black, eg_dp_white = [], [], []\n",
    "eg_eo, eg_eo_black, eg_eo_white = [], [], []\n",
    "eg_eoo, eg_eoo_black, eg_eoo_white = [], [], []\n",
    "eg_fprp, eg_fprp_black, eg_fprp_white = [], [], []\n",
    "eg_erp, eg_erp_black, eg_erp_white = [], [], []\n",
    "\n",
    "gs_dp, gs_dp_black, gs_dp_white = [], [], []\n",
    "gs_eo, gs_eo_black, gs_eo_white = [], [], []\n",
    "gs_eoo, gs_eoo_black, gs_eoo_white = [], [], []\n",
    "gs_fprp, gs_fprp_black, gs_fprp_white = [], [], []\n",
    "gs_erp, gs_erp_black, gs_erp_white = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score  repay_probability  race  repay_indices\n",
      "0       832              98.99     1              1\n",
      "1       724              97.08     1              0\n",
      "2       746              97.96     1              0\n",
      "3       475              20.06     1              0\n",
      "4       687              94.60     1              1\n",
      "...     ...                ...   ...            ...\n",
      "9995    716              96.62     1              0\n",
      "9996    721              96.90     1              1\n",
      "9997    439              13.95     1              0\n",
      "9998    802              98.79     1              1\n",
      "9999    324               1.20     0              0\n",
      "\n",
      "[10000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data = get_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the x values:  [[832   1]\n",
      " [724   1]\n",
      " [746   1]\n",
      " ...\n",
      " [439   1]\n",
      " [802   1]\n",
      " [324   0]] \n",
      "\n",
      "Here are the y values:  [1 0 0 ... 0 1 0]\n",
      "Sample weights are all equal.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, race_train, race_test, sample_weight_train, sample_weight_test = prep_data(data=data, test_size=0.3, weight_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifier trained below is:  svm_linear\n"
     ]
    }
   ],
   "source": [
    "print('The classifier trained below is: ', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'dt':\n",
    "    # Initialize classifier:\n",
    "    classifier = DecisionTreeClassifier()\n",
    "elif model_name == 'gnb':\n",
    "    classifier = GaussianNB()\n",
    "elif model_name == 'lgr':\n",
    "    # Reference: https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a\n",
    "    classifier = LogisticRegression()\n",
    "elif model_name == 'svm_linear':\n",
    "    # Reference: https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python\n",
    "    classifier = svm.SVC(kernel='linear', probability=True)\n",
    "elif model_name = 'gbt':\n",
    "    # Reference: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "    # Note: max_depth default is 3 but tune this parameter for best performance\n",
    "    classifier = GradientBoostingClassifier(n_estimators=100) \n",
    "else:\n",
    "    print('PROBLEM: input a specified classifier above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier and collect predictions\n",
    "NOTE: atm sample_weight are all 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "\n",
    "# Train the classifier:\n",
    "model = classifier.fit(X_train,y_train, sample_weight_train)\n",
    "\n",
    "# Make predictions with the classifier:\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "# Scores on test set\n",
    "test_scores = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of classifier overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.742333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.510095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.367202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.167679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.000917695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate difference</th>\n",
       "      <td>0.276165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True negative rate difference</th>\n",
       "      <td>0.278001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.278001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.276165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.278001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.926807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.0332061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Unmitigated\n",
       "Overall selection rate             0.742333\n",
       "Demographic parity difference      0.510095\n",
       "Demographic parity ratio           0.367202\n",
       "------                                     \n",
       "Overall balanced error rate        0.167679\n",
       "Balanced error rate difference  0.000917695\n",
       " ------                                    \n",
       "True positive rate difference      0.276165\n",
       "True negative rate difference      0.278001\n",
       "False positive rate difference     0.278001\n",
       "False negative rate difference     0.276165\n",
       "Equalized odds difference          0.278001\n",
       "  ------                                   \n",
       "Overall AUC                        0.926807\n",
       "AUC difference                    0.0332061"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics\n",
    "models_dict = {\"Unmitigated\": (y_predict, test_scores)}\n",
    "get_metrics_df(models_dict, y_test, race_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 614  218]\n",
      " [ 159 2009]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.74      0.77       832\n",
      "           1       0.90      0.93      0.91      2168\n",
      "\n",
      "    accuracy                           0.87      3000\n",
      "   macro avg       0.85      0.83      0.84      3000\n",
      "weighted avg       0.87      0.87      0.87      3000\n",
      "\n",
      "F1 score micro: \n",
      "0.8743333333333333\n",
      "F1 score weighted: \n",
      "0.8728670685677226\n",
      "F1 score binary: \n",
      "0.9142207053469852\n",
      "\n",
      "Selection Rate Overall:  0.7423333333333333\n",
      "TNR=TN/(TN+FP)=  0.7379807692307693\n",
      "TPR=TP/(FP+FN)=  0.926660516605166\n",
      "FNER=FN/(FN+TP)=  0.07333948339483395\n",
      "FPER=FP/(FP+TN)=  0.2620192307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The positional argument 'metric' has been replaced by a keyword argument 'metrics'. From version 0.10.0 passing it as a positional argument or as a keyword argument 'metric' will result in an error\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)\n",
    "results_dict = classification_report(y_test, y_predict, output_dict=True)\n",
    "print(classification_report(y_test, y_predict))\n",
    "# Add accuracy to the results list\n",
    "unmitigated.append(round(results_dict['accuracy']*100, 2))\n",
    "f1_micro, f1_weighted, f1_binary = get_f1_scores(y_test, y_predict)\n",
    "f1_str = str(round(f1_micro*100, 2))+\"/\"+str(round(f1_weighted*100, 2))+\"/\"+str(round(f1_binary*100, 2))\n",
    "# Add f1 scores to results list\n",
    "unmitigated.append(f1_str)\n",
    "# Add Selection rate to results list\n",
    "sr = get_selection_rates(y_test, y_predict, race_test, 0)\n",
    "unmitigated.append(round(sr*100, 2))\n",
    "# Add Outcome rates to results list\n",
    "tnr, tpr, fner, fper = evaluation_outcome_rates(y_test, y_predict, sample_weight_test)\n",
    "unmitigated.append(round(tnr*100, 2))\n",
    "unmitigated.append(round(tpr*100, 2))\n",
    "unmitigated.append(round(fner*100, 2))\n",
    "unmitigated.append(round(fper*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed impact calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The delayed impact of the black group is:  13.8\n",
      "The delayed impact of the white group is:  42.97142857142857\n"
     ]
    }
   ],
   "source": [
    "di_black, di_white = calculate_delayed_impact(X_test, y_test, y_predict, race_test)\n",
    "# Add DI to results list\n",
    "di_str = str(round(di_black, 2))+\"/\"+str(round(di_white, 2))\n",
    "unmitigated.append(di_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metric Evaluation of classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP Difference:  0.5100952380952382\n",
      "-->difference of 0 means that all groups have the same selection rate\n",
      "DP Ratio: 0.3672022684310019\n",
      "-->ratio of 1 means that all groups have the same selection rate \n",
      "\n",
      "EOD Difference:  0.2780008666762964\n",
      "-->difference of 0 means that all groups have the same TN, TN, FP, and FN rates\n",
      "EOD Ratio: 0.17962489343563512\n",
      "-->ratio of 1 means that all groups have the same TN, TN, FP, and FN rates rates \n",
      "\n",
      "EOO/TPR Difference:  0.27616547633252087\n",
      "FPR Difference:  0.2780008666762964\n",
      "ER Difference:  0.04533333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dp_diff, eod_diff, eoo_dif, fpr_dif, er_dif = print_fairness_metrics(y_test, y_predict, race_test, sample_weight_test)\n",
    "\n",
    "# Add the fairness metric differences to results list\n",
    "unmitigated.append(round(dp_diff*100, 2))\n",
    "unmitigated.append(round(eod_diff*100, 2))\n",
    "unmitigated.append(round(eoo_dif*100, 2))\n",
    "unmitigated.append(round(fpr_dif*100, 2))\n",
    "unmitigated.append(round(er_dif*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of classifier by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION FOR BLACK GROUP\n",
      "[[216  14]\n",
      " [ 48  97]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.87       230\n",
      "           1       0.87      0.67      0.76       145\n",
      "\n",
      "    accuracy                           0.83       375\n",
      "   macro avg       0.85      0.80      0.82       375\n",
      "weighted avg       0.84      0.83      0.83       375\n",
      "\n",
      "F1 score micro: \n",
      "0.8346666666666667\n",
      "F1 score weighted: \n",
      "0.8293771086369772\n",
      "F1 score binary: \n",
      "0.7578125\n",
      "\n",
      "TNR=TN/(TN+FP)=  0.9391304347826087\n",
      "TPR=TP/(FP+FN)=  0.6689655172413793\n",
      "FNER=FN/(FN+TP)=  0.3310344827586207\n",
      "FPER=FP/(FP+TN)=  0.06086956521739131\n",
      "\n",
      "EVALUATION FOR WHITE GROUP\n",
      "[[ 398  204]\n",
      " [ 111 1912]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.72       602\n",
      "           1       0.90      0.95      0.92      2023\n",
      "\n",
      "    accuracy                           0.88      2625\n",
      "   macro avg       0.84      0.80      0.82      2625\n",
      "weighted avg       0.88      0.88      0.88      2625\n",
      "\n",
      "F1 score micro: \n",
      "0.88\n",
      "F1 score weighted: \n",
      "0.8763256494772453\n",
      "F1 score binary: \n",
      "0.9238946605460256\n",
      "\n",
      "TNR=TN/(TN+FP)=  0.6611295681063123\n",
      "TPR=TP/(FP+FN)=  0.9451309935739002\n",
      "FNER=FN/(FN+TP)=  0.054869006426099855\n",
      "FPER=FP/(FP+TN)=  0.3388704318936877\n",
      "Selection Rate By Group:  sensitive_feature_0\n",
      "0       0.296\n",
      "1    0.806095\n",
      "Name: selection_rate, dtype: object \n",
      "\n",
      "The delayed impact of the black group is:  13.8\n",
      "The delayed impact of the white group is:  42.97142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The positional argument 'metric' has been replaced by a keyword argument 'metrics'. From version 0.10.0 passing it as a positional argument or as a keyword argument 'metric' will result in an error\n"
     ]
    }
   ],
   "source": [
    "results_black, results_white = evaluation_by_race(X_test, y_test, race_test, y_predict, sample_weight_test)\n",
    "unmitigated_black.extend(results_black)\n",
    "unmitigated_white.extend(results_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unmitigated': [87.43, '87.43/87.29/91.42', 74.23, 73.8, 92.67, 7.33, 26.2, '13.8/42.97', 51.01, 27.8, 27.62, 27.8, 4.53]}\n",
      "{'Unmitigated': [83.47, '83.47/82.94/75.78', 29.6, 93.91, 66.9, 33.1, 6.09, 13.8]}\n",
      "{'Unmitigated': [88.0, '88.0/87.63/92.39', 80.61, 66.11, 94.51, 5.49, 33.89, 42.97]}\n"
     ]
    }
   ],
   "source": [
    "run_key = 'Unmitigated'\n",
    "overall_results_dict = add_values_in_dict({}, run_key, unmitigated)\n",
    "black_results_dict = add_values_in_dict({}, run_key, unmitigated_black)\n",
    "white_results_dict = add_values_in_dict({}, run_key, unmitigated_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentiated Gradient Reduction Alg for Adding Fairness Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 60em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 60em; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated Gradient Reduction Alg is used here with  DP  as the fairness constraint.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'DP', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = 'EG DP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'EO', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = 'EG EO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOO (True Positive Rate Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'TPRP', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = 'EG EOO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'FPRP', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = 'EG FPRP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Rate Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'ERP', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = 'EG ERP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounded Group Loss (issue, need to figure out loss parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'BGL', 'EG', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, False)\n",
    "run_key = 'EG DP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Reduction Alg for Adding Fairness Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'DP', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "lambda_vecs = mitigator.lambda_vecs_\n",
    "print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells come from: https://github.com/fairlearn/fairlearn/blob/main/notebooks/Binary%20Classification%20with%20the%20UCI%20Credit-card%20Default%20Dataset.ipynb\n",
    "\n",
    "Note: we train multiple models corresponding to different trade-off points between the performance metric (balanced accuracy) and fairness metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_show(mitigator, demographic_parity_difference, y_predict, X_test, y_test, race_test, 'DemParityDifference','GS DPD', models_dict, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict.pop('GS DPD')\n",
    "models_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = 'GS DP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalized Odds Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'EO', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "lambda_vecs = mitigator.lambda_vecs_\n",
    "print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_show(mitigator, equalized_odds_difference, y_predict, X_test, y_test, race_test, 'EOddsDifference','GS EO', models_dict, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict.pop('GS EO')\n",
    "models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = 'GS EO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOO (True Positive Rate Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'TPRP', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "lambda_vecs = mitigator.lambda_vecs_\n",
    "print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_show(mitigator, true_positive_rate_difference, y_predict, X_test, y_test, race_test, 'TPRPDifference','GS TPRP', models_dict, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict.pop('GS TPRP')\n",
    "models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = 'GS EOO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'FPRP', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "lambda_vecs = mitigator.lambda_vecs_\n",
    "print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the below models are the same for DT classifier!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_show(mitigator, false_positive_rate_difference, y_predict, X_test, y_test, race_test, 'FPRPDifference','GS FPRP', models_dict, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict.pop('GS FPRP')\n",
    "models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = 'GS FPRP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Rate Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'ERP', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "lambda_vecs = mitigator.lambda_vecs_\n",
    "print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairlearn doesnt have an erp difference metric for the below\n",
    "#grid_search_show(gs_erp, error_difference, y_predict, X_test, y_test, race_test, 'ERDifference','GS ERP', models_dict, 0.3)\n",
    "#models_dict.pop('GS FPRP')\n",
    "#models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_key = 'GS ERP Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounded Group Loss (issue, need to figure out loss parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mitigator, results_overall, results_black, results_white= add_constraint(model, 'BGL', 'GS', X_train, y_train, race_train, race_test, X_test, y_test, y_predict, sample_weight_test, False)\n",
    "run_key = 'GS EO Mitigated'\n",
    "overall_results_dict = add_values_in_dict(overall_results_dict, run_key, results_overall)\n",
    "black_results_dict = add_values_in_dict(black_results_dict, run_key, results_black)\n",
    "white_results_dict = add_values_in_dict(white_results_dict, run_key, results_white)\n",
    "print(overall_results_dict)\n",
    "print(black_results_dict)\n",
    "print(white_results_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the values of lambda_i chosen for us:\n",
    "#lambda_vecs = gs_dp.lambda_vecs_\n",
    "#print(lambda_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Save results to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use below!!\n",
    "overall_fieldnames = ['Run', 'Acc', 'F1micro/F1w/F1bsr', 'SelectionRate', 'TNR rate', 'TPR rate', 'FNER', 'FPER', 'DIB/DIW', 'DP Diff', 'EO Diff', 'TPR Diff', 'FPR Diff', 'ER Diff']\n",
    "byrace_fieldnames = ['Run', 'Acc', 'F1micro/F1w/F1bsr', 'SelectionRate', 'TNR rate', 'TPR rate', 'FNER', 'FPER', 'DIB/DIW']\n",
    "save_dict_2_csv(overall_results_dict, overall_fieldnames, model_name+'_overall_results.csv')\n",
    "save_dict_2_csv(black_results_dict, byrace_fieldnames, model_name+'_black_results.csv')\n",
    "save_dict_2_csv(white_results_dict, byrace_fieldnames, model_name+'_white_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
