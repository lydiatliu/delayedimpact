{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score  repay_probability  race  repay_indices\n",
      "0       610              78.90     1              1\n",
      "1       568              47.77     0              0\n",
      "2       750              98.13     1              1\n",
      "3       775              98.45     1              1\n",
      "4       704              95.88     1              1\n",
      "...     ...                ...   ...            ...\n",
      "9995    832              98.99     1              1\n",
      "9996    416              10.91     1              0\n",
      "9997    444              14.63     1              0\n",
      "9998    778              98.47     1              1\n",
      "9999    738              97.68     1              1\n",
      "\n",
      "[10000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/home/mackenzie/git_repositories/delayedimpact/data/simData_oom10.csv')\n",
    "data[['score', 'race']] = data[['score', 'race']].astype(int)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Make data into train/test form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['score', 'race']].values\n",
    "y = data['repay_indices'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# collect our sensitive attribute\n",
    "race_train = X_train[:, 1]\n",
    "race_test = X_test[:, 1]\n",
    "\n",
    "# for fairlearn mitigator algs to work, I have to weigh the data\n",
    "# for now I'm weighing everything the same\n",
    "# TODO: add correct sample weights according to, http://www.surveystar.com/startips/weighting.pdf\n",
    "#       and https://www.nlsinfo.org/content/cohorts/nlsy97/using-and-understanding-the-data/sample-weights-design-effects/page/0/0/#intro\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "#sample_weight[y_train[:,1] == 0] = 1.5 \n",
    "\n",
    "# Below example from: https://androidkt.com/set-sample-weight-in-keras/\n",
    "#sample_weight[y_train == 3] = 1.5\n",
    "\n",
    "# Q: do I need to scale the data??\n",
    "# Standardize features by removing mean and scaling to unit variance:\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X_train)\n",
    "#X_train = scaler.transform(X_train)\n",
    "#X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_by_race(X_test, y_test, y_predict):\n",
    "    #races_test = X_test[:, 1]\n",
    "    \n",
    "    y_test_black = []\n",
    "    y_pred_black = []\n",
    "    y_test_white = []\n",
    "    y_pred_white = []\n",
    "\n",
    "    # splitting up the y_test and y_pred values by race to then use for race specific classification reports\n",
    "    for index, race in enumerate(race_test):\n",
    "        if(race == 0):  # black\n",
    "            y_test_black.append(y_test[index])\n",
    "            y_pred_black.append(y_predict[index])\n",
    "        elif(race == 1):  # white\n",
    "            y_test_white.append(y_test[index])\n",
    "            y_pred_white.append(y_predict[index])\n",
    "        else:\n",
    "            print('You should not end up here...')\n",
    "            \n",
    "    print('EVALUATION FOR BLACK GROUP')\n",
    "    print(confusion_matrix(y_test_black, y_pred_black))\n",
    "    print(classification_report(y_test_black, y_pred_black)) \n",
    "    \n",
    "    print('EVALUATION FOR WHITE GROUP')\n",
    "    print(confusion_matrix(y_test_white, y_pred_white))\n",
    "    print(classification_report(y_test_white, y_pred_white)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://fairlearn.org/v0.5.0/api_reference/fairlearn.metrics.html\n",
    "\n",
    "def print_fairness_metrics(y_true, y_pred, sensitive_features, sample_weight=None):\n",
    "    sr_mitigated = MetricFrame(metric=selection_rate, y_true=y_test, y_pred=y_pred_mitigated, sensitive_features=race_test)\n",
    "    print('Selection Rate Overall: ', sr_mitigated.overall)\n",
    "    print('Selection Rate By Group: ',sr_mitigated.by_group, '\\n')\n",
    "\n",
    "    # Note: difference of 0 means that all groups have the same selection rate\n",
    "    dp_diff = demographic_parity_difference(y_true=y_test, y_pred=y_pred_mitigated, sensitive_features=race_test)\n",
    "    print('DP Difference: ', dp_diff)\n",
    "    # Note: ratio of 1 means that all groups have the same selection rate\n",
    "    dp_ratio = demographic_parity_ratio(y_true=y_test, y_pred=y_pred_mitigated, sensitive_features=race_test)\n",
    "    print('DP Ratio:', dp_ratio, '\\n')\n",
    "    \n",
    "    # Note: difference of 0 means that all groups have the same true positive, true negative, false positive, and false negative rates.\n",
    "    eod_diff = equalized_odds_difference(y_true=y_test, y_pred=y_pred_mitigated, sensitive_features=race_test)\n",
    "    print('EOD Difference: ', eod_diff)\n",
    "    # Note: ratio of 1 means that all groups have the same true positive, true negative, false positive, and false negative rates.\n",
    "    eod_ratio = equalized_odds_ratio(y_true=y_test, y_pred=y_pred_mitigated, sensitive_features=race_test)\n",
    "    print('EOD Ratio:', eod_ratio, '\\n')\n",
    "    \n",
    "    # the below are overall metrics, similar to what I get in my confusion matrices above\n",
    "    print('The below metrics are for overall:')\n",
    "    \n",
    "    # for the below I can add a 'sample_weight' parameter\n",
    "    fner = false_negative_rate(y_true, y_pred, pos_label=1)\n",
    "    print('FNER', fner)\n",
    "    fper = false_positive_rate(y_true, y_pred, pos_label=1)\n",
    "    print('FPER', fper)\n",
    "    tnr = true_negative_rate(y_true, y_pred, pos_label=1)\n",
    "    print('TNR', tnr)\n",
    "    tpr = true_positive_rate(y_true, y_pred, pos_label=1)\n",
    "    print('TPR', tpr)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS:\n",
    "1. Get fairlearn widget to work (see what msr fairlearn dude responds to my GH issue comment)\n",
    "2. Add fairness metric evaluation to the baseline model\n",
    "3. Get confusion matrices for after the reduction alg is used\n",
    "4. Add other fairness metric constraints to the reduction alg\n",
    "5. Construct a function/for loop of sorts so that I'm not repeating code \n",
    "6. Try the other reduction algorithm\n",
    "7. Figure out what other baseline models I can use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train+Test Gaussian Naive Bayes classifier (Fairlearn used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifier:\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier:\n",
    "model = gnb.fit(X_train, y_train, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 617  232]\n",
      " [ 188 1963]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       849\n",
      "           1       0.89      0.91      0.90      2151\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.83      0.82      0.82      3000\n",
      "weighted avg       0.86      0.86      0.86      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the classifier:\n",
    "y_predict = gnb.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_predict))\n",
    "print(classification_report(y_test, y_predict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION FOR BLACK GROUP\n",
      "[[233   0]\n",
      " [122   4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79       233\n",
      "           1       1.00      0.03      0.06       126\n",
      "\n",
      "    accuracy                           0.66       359\n",
      "   macro avg       0.83      0.52      0.43       359\n",
      "weighted avg       0.78      0.66      0.54       359\n",
      "\n",
      "EVALUATION FOR WHITE GROUP\n",
      "[[ 384  232]\n",
      " [  66 1959]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.62      0.72       616\n",
      "           1       0.89      0.97      0.93      2025\n",
      "\n",
      "    accuracy                           0.89      2641\n",
      "   macro avg       0.87      0.80      0.82      2641\n",
      "weighted avg       0.88      0.89      0.88      2641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_by_race(X_test, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use MSR Fairlearn to add a fairness constraint to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection Rate Overall:  0.492\n",
      "Selection Rate By Group:  sensitive_feature_0\n",
      "0    0.481894\n",
      "1    0.493374\n",
      "Name: selection_rate, dtype: object \n",
      "\n",
      "DP Difference:  0.011479571657144305\n",
      "DP Ratio: 0.9767325028806462 \n",
      "\n",
      "EOD Difference:  0.3240413020455939\n",
      "EOD Ratio: 0.36553257666703043 \n",
      "\n",
      "The below metrics are for overall:\n",
      "FNER 0.4225941422594142\n",
      "FPER 0.2756183745583039\n",
      "TNR 0.7243816254416962\n",
      "TPR 0.5774058577405857\n"
     ]
    }
   ],
   "source": [
    "# Reference for this cell's code: https://fairlearn.org/main/quickstart.html\n",
    "# Reduction Algs explained here: https://fairlearn.org/main/user_guide/mitigation.html#reductions\n",
    "\n",
    "# TODO--try Gridsearch reduction alg: https://fairlearn.org/main/user_guide/mitigation.html\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "from fairlearn.metrics import *\n",
    "np.random.seed(0)  # set seed for consistent results with ExponentiatedGradient\n",
    "\n",
    "constraint = DemographicParity()\n",
    "mitigator = ExponentiatedGradient(model, constraint)\n",
    "mitigator.fit(X_train, y_train, sensitive_features=race_train)\n",
    "y_pred_mitigated = mitigator.predict(X_test)\n",
    "\n",
    "# THINK: the below fairness metric stuff might be more useful for the original model??\n",
    "\n",
    "# I can plug in a bunch of different metrics to the metric parameter for evaluation\n",
    "\n",
    "# TODO: get confusion matrices for mitigated data/groups\n",
    "\n",
    "# Example code for getting fairness metrics from fairlearn\n",
    "print_fairness_metrics(y_true=y_test, y_pred=y_pred_mitigated, sensitive_features=race_test) # sample_weight is an optional parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf84bd9e559944e0932754428418df9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FairlearnWidget(value={'true_y': [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<fairlearn.widget._fairlearn_dashboard.FairlearnDashboard at 0x7fe9239acd30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: figure out how to get the widget to work, https://fairlearn.org/v0.6.2/api_reference/fairlearn.widget.html\n",
    "from fairlearn.widget import FairlearnDashboard\n",
    "FairlearnDashboard(sensitive_features=race_test,\n",
    "                   sensitive_feature_names=['race'],\n",
    "                   y_true=y_test,\n",
    "                   y_pred={\"initial model\": y_predict, \"mitigated model\": y_pred_mitigated}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-9e58c3ebf1f9>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-9e58c3ebf1f9>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    pip show fairlearn\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from fairlearn import show_versions\n",
    "show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fairlearn\n",
      "Version: 0.6.2\n",
      "Summary: Algorithms for mitigating unfairness in supervised machine learning\n",
      "Home-page: https://github.com/fairlearn/fairlearn\n",
      "Author: Miroslav Dudik, Richard Edgar, Brandon Horn, Roman Lutz\n",
      "Author-email: fairlearn@microsoft.com\n",
      "License: UNKNOWN\n",
      "Location: /home/mackenzie/.local/lib/python3.8/site-packages\n",
      "Requires: scikit-learn, scipy, pandas, numpy\n",
      "Required-by: parity-fairness\n",
      "---\n",
      "Name: jupyter\n",
      "Version: 1.0.0\n",
      "Summary: Jupyter metapackage. Install all the Jupyter components in one go.\n",
      "Home-page: http://jupyter.org\n",
      "Author: Jupyter Development Team\n",
      "Author-email: jupyter@googlegroups.org\n",
      "License: BSD\n",
      "Location: /home/mackenzie/.local/lib/python3.8/site-packages\n",
      "Requires: qtconsole, nbconvert, notebook, ipykernel, ipywidgets, jupyter-console\n",
      "Required-by: witwidget\n",
      "---\n",
      "Name: notebook\n",
      "Version: 6.4.0\n",
      "Summary: A web-based notebook environment for interactive computing\n",
      "Home-page: http://jupyter.org\n",
      "Author: Jupyter Development Team\n",
      "Author-email: jupyter@googlegroups.com\n",
      "License: BSD\n",
      "Location: /home/mackenzie/.local/lib/python3.8/site-packages\n",
      "Requires: jupyter-client, nbformat, terminado, nbconvert, traitlets, jinja2, prometheus-client, ipykernel, argon2-cffi, Send2Trash, tornado, ipython-genutils, jupyter-core, pyzmq\n",
      "Required-by: widgetsnbextension, jupyter, jupyter-http-over-ws\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show fairlearn jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train+Test KNN classifier\n",
    "Note: KNN does not use sample_weight in its fit function, so can't use fairlearn with it :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ref: https://www.activestate.com/resources/quick-reads/how-to-classify-data-in-python/\n",
    "\n",
    "# Use the KNN classifier to fit data:\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 634  215]\n",
      " [ 144 2007]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78       849\n",
      "           1       0.90      0.93      0.92      2151\n",
      "\n",
      "    accuracy                           0.88      3000\n",
      "   macro avg       0.86      0.84      0.85      3000\n",
      "weighted avg       0.88      0.88      0.88      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict y data with classifier: \n",
    "y_predict = classifier.predict(X_test)\n",
    "\n",
    "# Print results: \n",
    "print(confusion_matrix(y_test, y_predict))\n",
    "print(classification_report(y_test, y_predict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION FOR BLACK GROUP\n",
      "[[198  35]\n",
      " [ 30  96]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86       233\n",
      "           1       0.73      0.76      0.75       126\n",
      "\n",
      "    accuracy                           0.82       359\n",
      "   macro avg       0.80      0.81      0.80       359\n",
      "weighted avg       0.82      0.82      0.82       359\n",
      "\n",
      "EVALUATION FOR WHITE GROUP\n",
      "[[ 436  180]\n",
      " [ 114 1911]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.71      0.75       616\n",
      "           1       0.91      0.94      0.93      2025\n",
      "\n",
      "    accuracy                           0.89      2641\n",
      "   macro avg       0.85      0.83      0.84      2641\n",
      "weighted avg       0.89      0.89      0.89      2641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_by_race(X_test, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train+Test Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "\n",
    "# Initialize classifier:\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier:\n",
    "clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 617  232]\n",
      " [ 188 1963]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       849\n",
      "           1       0.89      0.91      0.90      2151\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.83      0.82      0.82      3000\n",
      "weighted avg       0.86      0.86      0.86      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the classifier:\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_predict))\n",
    "print(classification_report(y_test, y_predict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION FOR BLACK GROUP\n",
      "[[233   0]\n",
      " [122   4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79       233\n",
      "           1       1.00      0.03      0.06       126\n",
      "\n",
      "    accuracy                           0.66       359\n",
      "   macro avg       0.83      0.52      0.43       359\n",
      "weighted avg       0.78      0.66      0.54       359\n",
      "\n",
      "EVALUATION FOR WHITE GROUP\n",
      "[[ 384  232]\n",
      " [  66 1959]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.62      0.72       616\n",
      "           1       0.89      0.97      0.93      2025\n",
      "\n",
      "    accuracy                           0.89      2641\n",
      "   macro avg       0.87      0.80      0.82      2641\n",
      "weighted avg       0.88      0.89      0.88      2641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_by_race(X_test, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train+Test Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a\n",
    "\n",
    "# Instantiate classifier:\n",
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "# Train the classifier:\n",
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 617  232]\n",
      " [ 188 1963]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       849\n",
      "           1       0.89      0.91      0.90      2151\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.83      0.82      0.82      3000\n",
      "weighted avg       0.86      0.86      0.86      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the classifier:\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_predict))\n",
    "print(classification_report(y_test, y_predict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION FOR BLACK GROUP\n",
      "[[233   0]\n",
      " [122   4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79       233\n",
      "           1       1.00      0.03      0.06       126\n",
      "\n",
      "    accuracy                           0.66       359\n",
      "   macro avg       0.83      0.52      0.43       359\n",
      "weighted avg       0.78      0.66      0.54       359\n",
      "\n",
      "EVALUATION FOR WHITE GROUP\n",
      "[[ 384  232]\n",
      " [  66 1959]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.62      0.72       616\n",
      "           1       0.89      0.97      0.93      2025\n",
      "\n",
      "    accuracy                           0.89      2641\n",
      "   macro avg       0.87      0.80      0.82      2641\n",
      "weighted avg       0.88      0.89      0.88      2641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_by_race(X_test, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Train+Test Support Vector Machines\n",
    "\n",
    "Reference: https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate classifier:\n",
    "clf = svm.SVC(kernel='linear')  # can try other kernels\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 617  232]\n",
      " [ 188 1963]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       849\n",
      "           1       0.89      0.91      0.90      2151\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.83      0.82      0.82      3000\n",
      "weighted avg       0.86      0.86      0.86      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_predict))\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION FOR BLACK GROUP\n",
      "[[233   0]\n",
      " [122   4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79       233\n",
      "           1       1.00      0.03      0.06       126\n",
      "\n",
      "    accuracy                           0.66       359\n",
      "   macro avg       0.83      0.52      0.43       359\n",
      "weighted avg       0.78      0.66      0.54       359\n",
      "\n",
      "EVALUATION FOR WHITE GROUP\n",
      "[[ 384  232]\n",
      " [  66 1959]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.62      0.72       616\n",
      "           1       0.89      0.97      0.93      2025\n",
      "\n",
      "    accuracy                           0.89      2641\n",
      "   macro avg       0.87      0.80      0.82      2641\n",
      "weighted avg       0.88      0.89      0.88      2641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_by_race(X_test, y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Notes\n",
    "\n",
    "TODO: increase dataset oom even more\n",
    "TODO: try other svm kernels\n",
    "\n",
    "https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
